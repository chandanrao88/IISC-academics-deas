{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d89dcf56-c9de-4bf6-aa00-c2bc6781914b",
   "metadata": {},
   "source": [
    "#  This PySpark application efficiently reads streaming data from Kafka, applies multiple data transformation strategies to ensure data privacy and integrity, and writes the transformed data to Google BigQuery. The implementation demonstrates the use of various PySpark functions and UDFs to achieve the desired transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64040be7-6655-4cac-819e-1816c92b5b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading  necessary libraries\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, sha2, concat_ws, expr, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af67bb4f-26c3-4401-b8a6-2d15327a2093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToBigQuery\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,org.apache.kafka:kafka-clients:3.8.0\") \\\n",
    "    .config(\"parentProject\", \"team-plutus-iisc\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f854e19-1022-43ad-aeea-3bfbb7be7bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka topic and servers configuration\n",
    "kafka_bootstrap_servers = \"10.142.0.3:9092\"\n",
    "kafka_topic = \"visit-data-topic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b302c277-41c9-4225-ac3f-3e2059b56d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for parsing Kafka messages (assuming JSON format)\n",
    "message_schema = StructType([\n",
    "    StructField(\"hashed_device_id\", StringType(), True),\n",
    "    StructField(\"timezone_visit\", StringType(), True),\n",
    "    StructField(\"day_of_week_visit\", StringType(), True),\n",
    "    StructField(\"time_stamp\", StringType(), True),\n",
    "    StructField(\"lat_visit\", StringType(), True),\n",
    "    StructField(\"date_visit\", StringType(), True),\n",
    "    StructField(\"time_visit\", StringType(), True),\n",
    "    StructField(\"lon_visit\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd928b75-ed8e-4477-92fd-c5e2fdf66c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from Kafka topic\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"kafka.security.protocol\", \"PLAINTEXT\") \\\n",
    "    .option(\"auto.offset.reset\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87db9b09-b1ab-42ec-b8e7-b32348b1dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Kafka value (binary) to a string and parse the JSON\n",
    "parsed_df = kafka_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), message_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")  # Extract the individual fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd26306-a781-4b33-81cb-edc3293b6a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a simple substitution cipher for demonstration purposes\n",
    "def substitution_cipher(text):\n",
    "    return ''.join(chr((ord(char) + 3) % 256) for char in text)\n",
    "\n",
    "substitution_cipher_udf = udf(substitution_cipher, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71f22ca-ffb1-460d-bcd0-8bd510d7ea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation steps\n",
    "# Mask the hashed_device_id by applying a SHA-256 hash\n",
    "transformed_df = parsed_df.withColumn(\"masked_device_id_sha256\", sha2(col(\"hashed_device_id\"), 256))\n",
    "\n",
    "# Mask the hashed_device_id using a substitution cipher\n",
    "transformed_df = transformed_df.withColumn(\"masked_device_id_substitution\", substitution_cipher_udf(col(\"hashed_device_id\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174a9101-55c3-4989-8048-09f052ae5aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate latitude and longitude into a single column\n",
    "transformed_df = transformed_df.withColumn(\"location_concat\", concat_ws(\",\", col(\"lat_visit\"), col(\"lon_visit\")))\n",
    "\n",
    "# Obfuscate location by adding random noise\n",
    "def add_noise(value):\n",
    "    return value + random.uniform(-0.01, 0.01)\n",
    "\n",
    "add_noise_udf = udf(add_noise, DoubleType())\n",
    "\n",
    "transformed_df = transformed_df.withColumn(\"lat_visit_noisy\", add_noise_udf(col(\"lat_visit\").cast(DoubleType())))\n",
    "transformed_df = transformed_df.withColumn(\"lon_visit_noisy\", add_noise_udf(col(\"lon_visit\").cast(DoubleType())))\n",
    "transformed_df = transformed_df.withColumn(\"location_noisy\", concat_ws(\",\", col(\"lat_visit_noisy\"), col(\"lon_visit_noisy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e73fdf-bc0a-4b81-a515-b42eba114ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original latitude, longitude, and hashed_device_id columns\n",
    "transformed_df = transformed_df.drop(\"lat_visit\", \"lon_visit\", \"hashed_device_id\", \"lat_visit_noisy\", \"lon_visit_noisy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fa6b3d-dbe3-4eac-b23b-5de25b287672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigQuery configuration\n",
    "bigquery_table = \"team-plutus-iisc.location.location_visited\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071937ce-3bbc-4788-bc40-33a13b654587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write micro-batch to BigQuery\n",
    "def write_to_bigquery(df, epoch_id):\n",
    "    df.write \\\n",
    "        .format(\"com.google.cloud.spark.bigquery.v2.Spark34BigQueryTableProvider\") \\\n",
    "        .option(\"table\", bigquery_table) \\\n",
    "        .option(\"temporaryGcsBucket\", \"gs://visited-location/data/\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983725ca-c2a0-47dc-847f-023f5a14f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to BigQuery in micro-batches\n",
    "query = transformed_df \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(write_to_bigquery) \\\n",
    "    .option(\"project\", \"team-plutus-iisc\") \\\n",
    "    .option(\"checkpointLocation\", \"gs://visited-location/data/\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "# Await termination\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a889db-0fd0-41cd-8ffe-e5b52c9b1c40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
